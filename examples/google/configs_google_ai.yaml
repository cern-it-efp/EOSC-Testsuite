#---------------DEPLOYMENT SPECIFICATION FIELDS AND VARS------------------------
providerName: "google"
pathToKey: "/home/ipelu/.ssh/id_rsa"
pathToPubKey: "/home/ipelu/.ssh/id_rsa.pub"
openUser: openu

# Note having small VMs with GPUs may cause "ssh_exchange_identification: read: Connection reset by peer". Got that on master with n1-standard-16 and 2 v100
#flavor: "n1-standard-16" # ssh_exchange_identification with 2 v100
#flavor: "n1-standard-32" # 32 - 120 # worked fine with 4 v100
#flavor: "n1-highmem-16" # 16 - 104
#flavor: "n1-standard-64" # 64 - 240
#flavor: "n1-standard-96" # 96 - 360
flavor: "foo"

zone: "europe-west4-a"
#zone: "europe-west1-a"
#zone: "europe-west3-a"
#zone: "europe-north1-a"
#zone: "us-central1-a"

# gcloud compute images list --project deeplearning-platform-release --no-standard-images | grep -i "tf-1-15-cu"
#image: "ubuntu-os-cloud/ubuntu-1804-lts" # OK
#image: "deeplearning-platform-release/tf-1-15-cu110-v20201106-ubuntu-1804" # Failed to update apt cache: unknown reason
image: "deeplearning-platform-release/tf-1-15-cu110" # Failed to auto-install python-apt. Error was: 'E: Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?
#image: "deeplearning-platform-release/tf2-ent-2-3-cu110"


pathToCredentials: "/data/career/cern/clouds/gcp/cern-test-suite-fb46815ac1f0.json"
project: "cern-test-suite"

# NOTE: Instances with 1 NVIDIA Tesla V100 must have at most 12 vCPU cores. Either change the machine type or increase the number of GPUs. No limits when selecting 2 or more
#gpuType: "nvidia-tesla-p100"
gpuType: "nvidia-tesla-v100" # these can be used in us-central1

gpusPerNode: 4 # 1, 2, 4, 8 for v100 in us-central1 # 8 shows the ssh conn. error regardless of the VM size. 4 with std-32 worked fine for some time but ended up showing the ssh error too
storageCapacity: 200
