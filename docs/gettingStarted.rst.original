1. Getting started
---------------------------------------------
Please follow the steps below in order to deploy tests in a cloud provider:

Refer to section "Using Docker" to use the Docker image we provide to avoid dealing with required packages and dependencies.

1.1 Dependencies
==========================
This test-suite requires some packages to work properly and these must be installed by yourself directly. Please see below.

Terraform
^^^^^^^^^^^^^^^^
Terraform is the tool that creates the VMs that will later become a Kubernetes cluster. The test-suite makes use of it so download and
install |Terraform_link| on your machine.
In some cases, providers are not fully supported by Terraform, however they might provide plugins to bridge this gap. In such cases, please refer to the documentation of the provider to download the plugin.
Once downloaded, this must be placed at *~/.terraform.d/plugins* and execution permissions must be given to it (*+x*).

.. |Terraform_link| raw:: html

  <a href="https://learn.hashicorp.com/terraform/getting-started/install.html" target="_blank">Terraform</a>

Ansible
^^^^^^^^^^^^^^^^
Ansible is used to configure the VMs and install packages on them in order to bootstrap the cluster. Follow the steps |Ansible_link| to install it.

.. |Ansible_link| raw:: html

  <a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installing-ansible-with-pip" target="_blank">here</a>

Kubernetes client
^^^^^^^^^^^^^^^^^^^^^
In order to manage the Kubernetes cluster locally instead of using the master node, install |kubectl_link| on your machine.

.. |kubectl_link| raw:: html

  <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">kubectl</a>

Python
^^^^^^^^^
Python version 3 is required.
The following python packages are required:

- pyyaml

- jsonschema

- kubernetes

- requests

Please install them with pip3.

1.2 SSH key
==================
A ssh key pair is needed to establish connections to the VMs to be created later. Therefore, you must create (or import) this key on your provider
beforehand and place the private key at *~/.ssh/id_rsa*.
Note errors may occur if your key doesn't have the right permissions. Set these to the right value using the following command:

.. code-block:: console

    $ chmod 600 path/to/key

1.3 Security groups
==========================================
The following ports have to be opened:

.. list-table::
   :widths: 25 25 50
   :header-rows: 1

   * - Port
     - Protocol
     - Functionality
   * -
     - ICMP
     - Connectivity test
   * - 22
     - TCP
     - SSH
   * - 6443
     - TCP
     - Kubernetes API
   * - 10250
     - TCP
     - API which allows node access
   * - 8472
     - UDP
     - Flannel overlay network, k8s pods communication


1.4 Networking and IPs
==========================================
Some providers do not allocate public IPs to the VMs but use NAT. Hence the VM can be reached from outside but that IP is not really residing on the VM. This causes
conflicts when creating the Kubernetes cluster. If one wants to run the Test-Suite on a provider of this case, then the suite must be launched from within the
network the nodes will be connected to, this is a private network. In other words, **a VM will have to be created first manually** and the Test Suite will have to be
triggered from there.

1.5 Clone and prepare
==========================================
Cloning repository
^^^^^^^^^^^^^^^^^^^^^^^
Please clone the repository as follows and cd into it. This step is not needed when using the provided Docker image, the repository is already cloned there.

.. code-block:: console

    $ git clone https://github.com/cern-it-efp/EOSC-Testsuite.git
    $ cd EOSC-Testsuite


Configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Two YAML files have to be filled to configure the run. Examples of these two files can be found inside the examples folder.

``testsCatalog.yaml``

This file gathers details related to the tests that should be deployed.
Refer to the section "Test Catalog" to learn how to fill this file.

``configs.yaml``

This file gathers general details required to provision the infrastructure.
The file also contains a section named *costCalculation*. Refer to the section "Cost of run calculation" to understand how to fill that part.
The suite will create itself the Terraform files on the fly according to the configuration provided.

**General variables:**

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "azurerm". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster.
   * - openUser
     - User to be used for ssh connections.
   * - location
     - The region in which to create the compute instances. (required)
   * - subscriptionId
     - ID of the subscription. (required)
   * - resourceGroupName
     - Specifies the name of the Resource Group in which the Virtual Machine should exist. (required)
   * - pubSSH
     - Public SSH key of the key specified at configs.yaml's pathToKey. (required)


**Provider specific variables:**

``Azure``

(It is also possible to use AKS to provision the cluster, for this refer to section "Using existing clusters".)

Install az CLI and configure credentials with 'az login'.
Note resource group, security group, and subnet have to be created in advance.

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "azurerm". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster.
   * - openUser
     - User to be used for ssh connections.
   * - location
     - The region in which to create the compute instances. (required)
   * - subscriptionId
     - ID of the subscription. (required)
   * - resourceGroupName
     - Specifies the name of the Resource Group in which the Virtual Machine should exist. (required)
   * - pubSSH
     - Public SSH key of the key specified at configs.yaml's pathToKey. (required)
   * - securityGroupID
     - The ID of the Network Security Group to associate with the VMs's network interfaces (required)
   * - subnetId
     - Reference to a subnet in which the NIC for the VM has been created. (required)
   * - image.publisher
     - Specifies the publisher of the image used to create the virtual machines. (required)
   * - image.offer
     - Specifies the offer of the image used to create the virtual machines. (required)
   * - image.sku
     - Specifies the SKU of the image used to create the virtual machines. (required)
   * - image.version
     - Specifies the version of the image used to create the virtual machines. (required)


Note: the security group and subnet -virtual network too- have to be created beforehand and their ID's used at configs.yaml.
Also, if image's *publisher*, *offer*, *sku* and *version* are omitted, the following defaults will be used:

- publisher = OpenLogic

- offer = CentOS

- sku = 7.5

- version = latest

``AWS``

(It is also possible to use EKS to provision the cluster, for this refer to section "Using existing clusters".)

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "aws". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - openUser
     - User to be used for ssh connections. (required)
   * - region
     - The region in which to create the compute instances. (required)
   * - sharedCredentialsFile
     - The authentication method supported is AWS shared credential file. Specify here the absolute path to such file. (required)
   * - ami
     - AMI for the instances. (required)
   * - keyName
     - Name of the key for the instances. (required)


``GCP``

(It is also possible to use GKE to provision the cluster, for this refer to section "Using existing clusters". You will have to |use_gke| too.)

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "google". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - openUser
     - User to be used for ssh connections. Note VM specific keys are not supported, only project-wide SSH keys are.(required)
   * - zone
     - The zone in which to create the compute instances. (required)
   * - pathToCredentials
     - Path to the GCP JSON credentials file (note this file has to be downloaded in advance from the GCP console). (required)
   * - image
     - Image for the instances. (required)
   * - project
     - Google project under which the infrastructure has to be provisioned. (required)
   * - gpuType
     - Type of GPU to be used. Needed if the Deep Learning test was selected at testsCatalog.yaml.


.. |use_gke| raw:: html

  <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials?hl=en_US&_ga=2.141757301.-616534808.1554462142" target="_blank">fetch the kubectl kubeconfig file</a>

``OpenStack``

Regarding authentication, download the OpenStack RC File containing the credentials from the Horizon dashboard and source it.

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "openstack". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - storageCapacity
     - VM's disk size. The VMs will boot from disk, this sets the size of it. (required)
   * - imageID
     - OS Image ID to be used for the VMs. (required)
   * - keyPair
     - Name of the key to be used. Has to be created or imported beforehand. (required)
   * - openUser
     - User to be used for ssh connections. Root user will be used by default.
   * - securityGroups
     - Security groups array.
   * - region
     - The region in which to create the compute instances. If omitted, the region specified in the credentials file is used.
   * - availabilityZone
     - The availability zone in which to create the compute instances.
   * - networkName
     - Name of the newtork to be used.


``CloudStack``

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "cloudstack". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - openUser
     - User to be used for ssh connections. Root user will be used by default.
   * - keyPair
     - Name of the key to be used. Has to be created or imported beforehand. (required)
   * - securityGroups
     - Security groups array.
   * - zone
     - The zone in which to create the compute instances. (required)
   * - template
     - OS Image to be used for the VMs. (required)
   * - storageCapacity
     - VM's disk size.
   * - authFile
     - Path to the file containing the CloudStack credentials. See below the structure of such file. (required)

CloudStack credentials file's structure:

.. code-block:: console

  [cloudstack]
  url = your_api_url
  apikey = your_api_key
  secretkey = your_secret_key


``Exoscale``

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "exoscale". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - keyPair
     - Name of the key to be used. Has to be created or imported beforehand. (required)
   * - securityGroups
     - Security groups array.
   * - zone
     - The zone in which to create the compute instances. (required)
   * - template
     - OS Image to be used for the VMs. (required)
   * - storageCapacity
     - VM's disk size. (required)
   * - authFile
     - Path to the file containing the Exoscale credentials. See below the structure of such file. (required)


Exoscale credentials file's structure:

.. code-block:: console

  [cloudstack]
  key = EXOe3ca3e7621b7cd7a20f7e0de
  secret = 2_JvzFcZQL_Rg1nZSRNVheYQh9oYlL5aX3zX-eILiL4


``T-Systems' Open Telekom Cloud``

Note that to allow the VMs access the internet, Shared SNAT has to be enabled on the default VPC, which will be used for the suite run.

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "opentelekomcloud". (required)
   * - pathToKey
     - Path to the location of your private key, to be used for ssh connections. (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - keyPair
     - Name of the key to be used. Has to be created or imported beforehand. (required)
   * - securityGroups
     - Security groups array.
   * - storageCapacity
     - VM's disk size. The VMs will boot from disk, this sets the size of it. (required)
   * - authFile
     - Path to the yaml file containing the OTC credentials. See below the structure of such file. (required)
   * - imageID
     - ID of the image to be used on the VMs. (required)
   * - openUser
     - User to be used for ssh connections. (required)
   * - domainName
     - OTC Domain Name. (required)
   * - tenantName
     - OTC Tenant Name. (required)

Open Telekom Cloud credentials file's structure:

.. code-block:: console

  accK: 123456789abcd
  secK: 123456789abcd


``Oracle Cloud Infrastructure``

(It is also possible to use OKE to provision the cluster, for this refer to section "Using existing clusters".)

Variables for configs.yaml:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - providerName
     - It's value must be "oci". (required)
   * - pathToKey
     - Path to your private key, to be used for ssh connections. (required)
   * - ssh_public_key_path
     - Path to the public key belonging to your private key at pathToKey. This will be injected to the VMs (required)
   * - flavor
     - Flavor to be used for the main cluster. (required)
   * - storageCapacity
     - VM's disk size.
   * - authFile
     - Path to the yaml file containing the OTC credentials. See below the structure of such file. (required)
   * - image_ocid
     - The OCID of the image to be used on the VMs. (required)
   * - openUser
     - User to be used for ssh connections. (required)
   * - compartment_ocid
     - Compartment's OCID. (required)
   * - availability_domain
     - Availability domain to be used. (required)
   * - subnet_ocid
     - The OCID of the subnet to be used. (required)

Oracle Cloud Infrastructure credentials file's must be a YAML file containing only the following variables:

.. list-table::
   :widths: 25 50
   :header-rows: 1

   * - Name
     - Explanation / Values
   * - auth_private_key_path
     - Path to the private key to be used to authenticate to OCI. This is not the key to be used to ssh into the machines.
   * - user_ocid
     - User's OCID.
   * - tenancy_ocid
     - Tenancy's OCID.
   * - fingerprint
     - Authentication key's fingerprint.
   * - region
     - Region to be used.


1.6 Using Docker
===================
A Docker image has been built and pushed to Docker hub. This image allows you to skip section "Dependencies" and jump to "SSH key".

Run the container (pulls the image first):

.. code-block:: console

    $ docker run --net=host -it cernefp/tslauncher

Note the option '--net=host'. Without it, the container wouldn't be able to connect to the nodes, as it would not be in the same network as them and it is likely the nodes will not have public IPs. With that option, the container will use the network used by its host, which will be sharing the network with the nodes.

You will get a session on the container, directly inside the cloned repository.
